# 強化学習の一般論

## マルコフ決定過程(Markov Decision Process: MDP)

MDPは、以下の特徴を持つ。

マルコフ性: 

状態遷移がマルコフ過程に従い、現在の状態だけで将来の状態が確率的に決まる性質を持つ。過去の履歴に依存しないため、状態の遷移が単純化される。

決定性: 

意思決定者がある状態において、可能な行動を選択することによって、次の状態を決定する。意思決定者は、将来の報酬を最大化するために最適な行動を選択する。

確率性: 

状態遷移や報酬は確率的に決まる。つまり、同じ行動を選択しても、異なる状態に遷移し、異なる報酬を得ることがある。

MDPの構成要素は以下の４つである。

- 状態

- 行動

- 状態遷移の確率

- 即時報酬

MDPは強化学習の枠組みそのものであり、モデルベース強化学習はその枠組みを使用して環境モデルを構築し、モデルフリー強化学習はモデルを持たずに直接学習を行う。

<br/>
<br/>

# 強化学習の手法

## 動的計画法 (Dynamic Programming, DP）


環境の完全なモデルがマルコフ決定過程として与えられている場合にのみ適用できる方法である。全ての遷移の完全な確率分布を必要とするため、適応できる問題が限られる。


## モンテカルロ法

サンプル収益を平均化することに基づいて強化学習問題を解く方法である。現時点の方策πを用いてエピソードを生成し、その結果を用いて価値関数と方策を改善する。これを繰り返す。
モンテカルロ法やミニマックス法、アルファベータ法などをまとめて「探索」と呼び、TD学習を「強化学習」と呼ぶこともある。

## TD学習  (時間差分学習, temporal difference learning）

現在の状態価値関数の推定からブートストラップで学習するモデルフリーの強化学習の手法。TD学習の代表的な手法としてSarsaと[Q学習](https://qiita.com/naoya_ok/items/b2a14d5e1d45da4c1707)がある。

<br/>
<br/>


# このリポジトリのコード

## maze.ipynb
方策勾配法を用いたコード。
